\section{Design}

To simplify the problem, we focus on just the operation of (de)compression.
We will instrument a microservice in a recently proposed framework called Blueprint~\cite{anand2023blueprint}, which allows for easy microservice reconfiguration, and we will collect telemetry on the operation sizes for (de)compression.
We will offload (de)compression on Intel's In-Memory Analytics Accelerator (IAA).

When choosing service boundaries in an application architected as a collection of microservices, there is a tradeoff between splitting off a portion of the application as a distinct microservice and the incurred latency from the additional network communication.
We will characterize this tradeoff in the presence of a compression accelerator (Intel's IAA) by developing a toy microservice---a compressed object cache---and measuring the end-to-end latency, system throughput, and CPU utilization as we sweep static object sizes.

We will run this experiment for three different configurations: the monolithic application without (de)compression acceleration, the monolithic application with (de)compression acceleration, and a microservice configuration with a (de)compression service running with acceleration which is accessed remotely by the cache microservice (memcached).
We will analyze how different network latencies affect the tradeoff point, where the benefit of compression acceleration outweighs the cost of traversing the network for a remote call to the service.
After collecting the results, we can identify this tradeoff point and use it to program a rule in the centralized scheduler to redeploy the application with a remote compression microservice if the operation size telemetry signals a potential benefit.
