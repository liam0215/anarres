\section{Implementation}

Our final artifact include the following:
\begin{itemize}
    \item The Blueprint workflow code for our compressed object cache microservice.
    \item The Blueprint plugin code to wrap Intel's QPL library for utilizing the IAA (or performing (de)compression in software if the IAA is not present).
    \item The Blueprint scaffolding code for the monolithic and microservice configurations.
    \item The benchmark code for orchestrating the experiments and measuring the end-to-end latency, system throughput, and CPU utilization for the different deployments.
    \item Telemetry code for monitoring (de)compression operation size.
    \item Scheduler code for polling telemetry information and choosing to reconfigure and redeploy the application.
\end{itemize}

We detail the implementation of each component in the following sections.

\subsection{Blueprint Scaffolding}

\todo{scaffolding code for building the application as a single process or as separate Docker containers communicating over gRPC.}

For our evaluation, we will be running on two Intel Sapphire Rapids servers, one with the IAA enabled and one without.

\subsection{(De)compression Microservices}

We implemented a gRPC-based microservice that accepts requests for compression and decompression.
The service is written in Go using the Blueprint framework.
Each request includes a byte payload.
The service routes the request to either a CPU-based or IAA-based handler, depending on deployment configuration.

\subsection{QPL Plugin}

Using Go's Foreign Function Interface (FFI), we implemented a C++ shim layer to wrap Intel's QPL library.
In order to preserve Go's managed memory model, we manage our own memory allocation and deallocation for pointers passed to the shim layer and returned from it.

\subsection{Benchmark}

\todo{Things we measure}

\subsection{Telemetry and Scheduler}

\todo{gRPC. how scheduler polls. failed attempt to use opentelemetry and prometheus}
