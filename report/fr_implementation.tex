\section{Implementation}
To simplify the problem, we focus on just the operation of (de)compression.
We will instrument a microservice in a recently proposed framework called Blueprint~\cite{anand2023blueprint}, which allows for easy microservice reconfiguration, and we will collect telemetry on the operation sizes for (de)compression.
We will offload (de)compression on Intel's In-Memory Analytics Accelerator (IAA).

When choosing service boundaries in an application architected as a collection of microservices, there is a tradeoff between splitting off a portion of the application as a distinct microservice and the incurred latency from the additional network communication.
We will characterize this tradeoff in the presence of a compression accelerator (Intel's IAA) by developing a toy microservice---a compressed object cache---and measuring the end-to-end latency, system throughput, and CPU utilization as we sweep static object sizes.

We will run this experiment for three different configurations: the monolithic application without (de)compression acceleration, the monolithic application with (de)compression acceleration, and a microservice configuration with a (de)compression service running with acceleration which is accessed remotely by the cache microservice (memcached).
We will analyze how different network latencies affect the tradeoff point, where the benefit of compression acceleration outweighs the cost of traversing the network for a remote call to the service.
After collecting the results, we can identify this tradeoff point and use it to program a rule in the centralized scheduler to redeploy the application with a remote compression microservice if the operation size telemetry signals a potential benefit.

Our final artifact will include the following:
\begin{itemize}
    \item The Blueprint workflow code for our compressed object cache microservice.
    \item The Blueprint plugin code to wrap Intel's QPL library for utilizing the IAA (or performing (de)compression in software if the IAA is not present).
    \item The Blueprint scaffolding code for the monolithic and microservice configurations.
    \item The benchmark code for orchestrating the experiments and measuring the end-to-end latency, system throughput, and CPU utilization for the different deployments.
    \item Telemetry code for monitoring (de)compression operation size.
    \item Scheduler code for polling telemetry information and choosing to reconfigure and redeploy the application.
\end{itemize}

\subsection{Project Progress}

We have made significant progress on the project so far.
We spent substantial time getting familiar with Blueprint and reading through example microservices, but are now making good progress on the implementation.
\begin{itemize}
    \item We have implemented all of the Blueprint workflow code except for the actual calls to the compression library.
    \item We are currently implementing the Blueprint plugin code to wrap Intel's QPL library by implementing a C shim layer and using Go's FFI to call the shim layer.
    \item We have implemented the scaffolding code for building the application as a single process or as separate Docker containers communicating over gRPC.
    \item We have yet to implement the code to run the experiments and collect measurements.
    \item We have yet to implement the telemetry code for monitoring (de)compression operation size.
    \item We have yet to implement the scheduler code for polling telemetry information and making allocation decisions.
\end{itemize}
For our evaluation, we will be running on two Intel Sapphire Rapids servers, one with the IAA enabled and one without.
