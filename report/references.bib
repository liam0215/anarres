@inproceedings{kanev2015profiling,
  author    = {Svetoslav Kanev and Kim Hazelwood and Parthasarathy Ranganathan and Timothy M. Jones and Benjamin C. Lee and Christos Kozyrakis},
  title     = {Profiling a Warehouse-Scale Computer},
  booktitle = {Proceedings of the 42nd Annual International Symposium on Computer Architecture},
  pages     = {158--169},
  year      = {2015},
  doi       = {10.1145/2749469.2750392},
  publisher = {ACM}
}

@inproceedings{yuan2024intel,
  author    = {Yongqiang Yuan and Saeed Maleki and Jeff Demme and Dave Dunning and Mark D. Hill},
  title     = {Intel Accelerators Ecosystem: An SoC-Oriented Perspective: Industry Product},
  booktitle = {2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)},
  pages     = {848--862},
  year      = {2024},
  doi       = {10.1109/ISCA59077.2024.00066},
  publisher = {IEEE}
}

@misc{narayanan2023hetero,
  author    = {Deepak Narayanan and Kshitij Santhanam and Farzin Kazhamiaka and Amar Phanishayee and Matei Zaharia},
  title     = {Heterogeneity-Aware Cluster Scheduling Policies for Deep Learning Workloads},
  note      = {Unpublished manuscript}
}

@misc{cilium,
  title = {Cilium},
  note = {\url{https://cilium.io/}}
} 

@misc{otel,
title = {OpenTelemetry},
note = {\url{https://opentelemetry.io/}},
}

@misc{prometheus,
  title = {Prometheus},
  note = {\url{https://prometheus.io/}},
}

@inproceedings{subramanya2023sia,
  author    = {Siddhartha Jayaram Subramanya and Danish Arfeen and Shan Lin and Andrew Qiao and Zhen Jia and Gregory R. Ganger},
  title     = {Sia: Heterogeneity-Aware, Goodput-Optimized ML-Cluster Scheduling},
  booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
  pages     = {642--657},
  year      = {2023},
  doi       = {10.1145/3600006.3613175},
  publisher = {ACM}
}

@inproceedings{anand2023blueprint,
  author    = {Vikram Anand and Divya Garg and Andrew Kaufmann and Jonathan Mace},
  title     = {Blueprint: A Toolchain for Highly-Reconfigurable Microservice Applications},
  booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
  pages     = {482--497},
  year      = {2023},
  doi       = {10.1145/3600006.3613138},
  publisher = {ACM}
}

@misc{intelIntelQpl2025,
  title        = {Intel/Qpl},
  author       = {{Intel}},
  year         = {2025},
  month        = may,
  urldate      = {2025-06-13},
  abstract     = {Intel{\textregistered} Query Processing Library (Intel{\textregistered} QPL)},
  copyright    = {MIT},
  howpublished = {Intel{\textregistered} Corporation},
  note         = {\url{https://github.com/intel/qpl}}
}

@book{barrosoDatacenterComputer2019,
  title      = {The {{Datacenter}} as a {{Computer}}: {{Designing Warehouse-Scale Machines}}},
  shorttitle = {The {{Datacenter}} as a {{Computer}}},
  author     = {Barroso, Luiz Andr{\'e} and H{\"o}lzle, Urs and Ranganathan, Parthasarathy},
  year       = {2019},
  series     = {Synthesis {{Lectures}} on {{Computer Architecture}}},
  publisher  = {Springer International Publishing},
  address    = {Cham},
  urldate    = {2025-06-13},
  copyright  = {https://www.springer.com/tdm},
  isbn       = {978-3-031-00633-3 978-3-031-01761-2},
  langid     = {english}
}

@inproceedings{ghiasiSchedulingHeterogeneous2005,
  title     = {Scheduling for Heterogeneous Processors in Server Systems},
  booktitle = {Proceedings of the 2nd Conference on {{Computing}} Frontiers},
  author    = {Ghiasi, Soraya and Keller, Tom and Rawson, Freeman},
  year      = {2005},
  month     = may,
  series    = {{{CF}} '05},
  pages     = {199--210},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  doi       = {10.1145/1062261.1062295},
  urldate   = {2025-06-13},
  abstract  = {Applications on today's high-end systems typically make varying load demands over time. A single application may have many different phases during its lifetime, and workload mixes show interleaved phases. Memory-intensive work or phases may exhibit performance saturation at frequencies below the maximum possible for the processors due to the disparity between processor and memory speeds. Performance saturation is a sign of over-provisioning and leads to energy-inefficient systems. Computers using heterogeneous processors, with the same ISA, but different implementation details, have been proposed as a way of reducing power while avoiding or limiting performance degradation. However, using heterogeneous processors effectively is complicated and requires intelligent schedulingThe research reported here explores the use of a heterogeneous system of processors with identical ISAs and implementation details, but with differing voltages and frequencies. The scheduler uses the execution characteristics of each application to predict its future processing needs and then schedule it to a processor which matches those needs if one is available. The predictions are used to minimize the performance loss to the system as a whole rather than that of a single application. The result limits system power while minimizing total performance loss. A prototype implementation on a Power4 four-processor system is presented. The prototype scheduler is validated using both synthetic and real-world benchmarks. The prototype shows reasonable predictor accuracy and significant power savings for memory-bound applications},
  isbn      = {978-1-59593-019-4},
}

@article{menasceStaticDynamic1995,
  title    = {Static and {{Dynamic Processor Scheduling Disciplines}} in {{Heterogeneous Parallel Architectures}}},
  author   = {Menasce, D. A. and Saha, D. and Porto, S. C. D. and Almeida, V. A. F. and Tripathi, S. K.},
  year     = {1995},
  month    = jul,
  journal  = {Journal of Parallel and Distributed Computing},
  volume   = {28},
  number   = {1},
  pages    = {1--18},
  issn     = {0743-7315},
  doi      = {10.1006/jpdc.1995.1085},
  urldate  = {2025-06-13},
  abstract = {Most parallel jobs cannot be fully parallelized. In a homogeneous parallel machine-one in which all processors are identical-the serial fraction of the computation has to be executed at the speed of any of the identical processors, limiting the speedup that can be obtained due to parallelism. In a heterogeneous architecture, the sequential bottleneck can be greatly reduced by running the sequential part of the job or even the critical tasks in a faster processor. This paper uses Markov chain based models to analyze the performance of static and dynamic processor assignment policies for heterogeneous architectures. Parallel jobs are assumed to be described by acyclic directed task graphs. A new static processor assignment policy, called Largest Task First Minimum Finish Time (LTFMFT), is introduced. The analysis shows that this policy is very sensitive to the degree of heterogeneity of the architecture, and that it outperforms all other policies analyzed. Three dynamic assignment disciplines are compared and it is shown that, in heterogeneous environments, the disciplines that perform better are those that consider the structure of the task graph, and not only the service demands of the individual tasks. The performance of heterogeneous architectures is compared with cost-equivalent homogeneous ones taking into account different scheduling policies. Finally, static and dynamic processor assignment disciplines are compared in terms of performance.}
}

@inproceedings{topcuogluTaskScheduling1999,
  title     = {Task Scheduling Algorithms for Heterogeneous Processors},
  booktitle = {Proceedings. {{Eighth Heterogeneous Computing Workshop}} ({{HCW}}'99)},
  author    = {Topcuoglu, H. and Hariri, S. and Wu, Min-You},
  year      = {1999},
  month     = apr,
  pages     = {3--14},
  issn      = {1097-5209},
  doi       = {10.1109/HCW.1999.765092},
  urldate   = {2025-06-13},
  abstract  = {Scheduling computation tasks on processors is the key issue for high-performance computing. Although a large number of scheduling heuristics have been presented in the literature, most of them target only homogeneous resources. The existing algorithms for heterogeneous domains are not generally efficient because of their high complexity and/or the quality of the results. We present two low-complexity efficient heuristics, the Heterogeneous Earliest-Finish-Time (HEFT) algorithm and the Critical-Path-on-a-Processor (CPOP) algorithm for scheduling directed acyclic weighted task graphs (DAGs) on a bounded number of heterogeneous processors. We compared the performances of these algorithms against three previously proposed heuristics. The comparison study showed that our algorithms outperform previous approaches in terms of performance (schedule length ratio and speedup) and cost (time-complexity).},
  keywords  = {Computational efficiency,Costs,Processor scheduling,Scheduling algorithm},
}

@inproceedings{clarkProcessorAcceleration2003,
  title     = {Processor Acceleration through Automated Instruction Set Customization},
  booktitle = {Proceedings. 36th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}, 2003. {{MICRO-36}}.},
  author    = {Clark, N. and Zhong, Hongtao and Mahlke, S.},
  year      = {2003},
  month     = dec,
  pages     = {129--140},
  doi       = {10.1109/MICRO.2003.1253189},
  urldate   = {2025-06-13},
  abstract  = {Application-specific extensions to the computational capabilities of a processor provide an efficient mechanism to meet the growing performance and power demands of embedded applications. Hardware, in the form of new function units (or co-processors), and the corresponding instructions, are added to a baseline processor to meet the critical computational demands of a target application. The central challenge with this approach is the large degree of human effort required to identify and create the custom hardware units, as well as porting the application to the extended processor. In this paper, we present the design of a system to automate the instruction set customization process. A dataflow graph design space exploration engine efficiently identifies profitable computation subgraphs from which to create custom hardware, without artificially constraining their size or shape. The system also contains a compiler subgraph matching framework that identifies opportunities to exploit and generalize the hardware to support more computation graphs. We demonstrate the effectiveness of this system across a range of application domains and study the applicability of the custom hardware across the domain.},
  keywords  = {Acceleration,Computer aided instruction,Coprocessors,Embedded computing,Engines,Hardware,Humans,Power demand,Shape,Space exploration},
}

@inproceedings{augonnetDataAwareTask2010,
  title     = {Data-{{Aware Task Scheduling}} on {{Multi-accelerator Based Platforms}}},
  booktitle = {2010 {{IEEE}} 16th {{International Conference}} on {{Parallel}} and {{Distributed Systems}}},
  author    = {Augonnet, Cedric and {Clet-Ortega}, Jerome and Thibault, Samuel and Namyst, Raymond},
  year      = {2010},
  month     = dec,
  pages     = {291--298},
  issn      = {1521-9097},
  doi       = {10.1109/ICPADS.2010.129},
  urldate   = {2025-06-13},
  abstract  = {To fully tap into the potential of heterogeneous machines composed of multicore processors and multiple accelerators, simple offloading approaches in which the main trunk of the application runs on regular cores while only specific parts are offloaded on accelerators are not sufficient. The real challenge is to build systems where the application would permanently spread across the entire machine, that is, where parallel tasks would be dynamically scheduled over the full set of available processing units. To face this challenge, we previously proposed StarPU, a runtime system capable of scheduling tasks over multicore machines equipped with GPU accelerators. StarPU uses a software virtual shared memory (VSM) that provides a highlevel programming interface and automates data transfers between processing units so as to enable a dynamic scheduling of tasks. We now present how we have extended StarPU to minimize the cost of transfers between processing units in order to efficiently cope with multi-GPU hardware configurations. To this end, our runtime system implements data prefetching based on asynchronous data transfers, and uses data transfer cost prediction to influence the decisions taken by the task scheduler. We demonstrate the relevance of our approach by benchmarking two parallel numerical algorithms using our runtime system. We obtain significant speedups and high efficiency over multicore machines equipped with multiple accelerators. We also evaluate the behaviour of these applications over clusters featuring multiple GPUs per node, showing how our runtime system can combine with MPI.},
  keywords  = {Graphics processing unit,Libraries,Prefetching,Programming,Runtime,Semantics},
}

@article{guptaPegasusCoordinated2011,
  title    = {Pegasus: {{Coordinated Scheduling}} for {{Virtualized Accelerator-based Systems}}},
  author   = {Gupta, Vishakha and Schwan, Karsten and Tolia, Niraj},
  year     = {2011},
  journal  = {USENIX ATC},
  abstract = {Heterogeneous multi-cores---platforms comprised of both general purpose and accelerator cores---are becoming increasingly common. While applications wish to freely utilize all cores present on such platforms, operating systems continue to view accelerators as specialized devices. The Pegasus system described in this paper uses an alternative approach that offers a uniform resource usage model for all cores on heterogeneous chip multiprocessors. Operating at the hypervisor level, its novel scheduling methods fairly and efficiently share accelerators across multiple virtual machines, thereby making accelerators into first class schedulable entities of choice for many-core applications. Using NVIDIA GPGPUs coupled with x86-based general purpose host cores, a Xen-based implementation of Pegasus demonstrates improved performance for applications by better managing combined platform resources. With moderate virtualization penalties, performance improvements range from 18\% to 140\% over base GPU driver scheduling when the GPUs are shared.},
  langid   = {english},
}

@inproceedings{huangCoSAScheduling2021,
  title      = {{{CoSA}}: {{Scheduling}} by {{Constrained Optimization}} for {{Spatial Accelerators}}},
  shorttitle = {{{CoSA}}},
  booktitle  = {2021 {{ACM}}/{{IEEE}} 48th {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author     = {Huang, Qijing and Kang, Minwoo and Dinh, Grace and Norell, Thomas and Kalaiah, Aravind and Demmel, James and Wawrzynek, John and Shao, Yakun Sophia},
  year       = {2021},
  month      = jun,
  pages      = {554--566},
  issn       = {2575-713X},
  doi        = {10.1109/ISCA52012.2021.00050},
  urldate    = {2025-06-13},
  abstract   = {Recent advances in Deep Neural Networks (DNNs) have led to active development of specialized DNN accelerators, many of which feature a large number of processing elements laid out spatially, together with a multi-level memory hierarchy and flexible interconnect. While DNN accelerators can take advantage of data reuse and achieve high peak throughput, they also expose a large number of runtime parameters to the programmers who need to explicitly manage how computation is scheduled both spatially and temporally. In fact, different scheduling choices can lead to wide variations in performance and efficiency, motivating the need for a fast and efficient search strategy to navigate the vast scheduling space.To address this challenge, we present CoSA, a constrained-optimization-based approach for scheduling DNN accelerators. As opposed to existing approaches that either rely on designers' heuristics or iterative methods to navigate the search space, CoSA expresses scheduling decisions as a constrained-optimization problem that can be deterministically solved using mathematical optimization techniques. Specifically, CoSA leverages the regularities in DNN operators and hardware to formulate the DNN scheduling space into a mixed-integer programming (MIP) problem with algorithmic and architectural constraints, which can be solved to automatically generate a highly efficient schedule in one shot. We demonstrate that CoSA-generated schedules significantly outperform state-of-the-art approaches by a geometric mean of up to 2.5{\texttimes} across a wide range of DNN networks while improving the time-to-solution by 90{\texttimes}.},
  keywords   = {accelerator,compiler optimizations,Navigation,neural networks,Processor scheduling,Programming,Runtime,Schedules,scheduling,Search problems,Throughput},
}

@article{panneerselvamOperatingSystems2012,
  title    = {Operating {{Systems Should Manage Accelerators}}},
  author   = {Panneerselvam, Sankaralingam and Swift, Michael M},
  year     = {2012},
  journal  = {The 4th USENIX Workshop on Hot Topics in Parallelism},
  abstract = {The inexorable demand for computing power has lead to increasing interest in accelerator-based designs. An accelerator is specialized hardware unit that can perform a set of tasks with much higher performance or power efficiency than a general-purpose CPU. They may be embedded in the pipeline as a functional unit, as in SIMD instructions, or attached to the system as a separate device, as in a cryptographic co-processor.},
  langid   = {english},
}

@inproceedings{liuE3EnergyEfficient2019,
  title      = {E3: \{\vphantom\}{{Energy-Efficient}}\vphantom\{\} {{Microservices}} on \{\vphantom\}{{SmartNIC-Accelerated}}\vphantom\{\} {{Servers}}},
  shorttitle = {E3},
  booktitle  = {2019 {{USENIX Annual Technical Conference}} ({{USENIX ATC}} 19)},
  author     = {Liu, Ming and Peter, Simon and Krishnamurthy, Arvind and Phothilimthana, Phitchaya Mangpo},
  year       = {2019},
  pages      = {363--378},
  urldate    = {2025-06-14},
  isbn       = {978-1-939133-03-8},
  langid     = {english},
  note       = {\url{https://www.usenix.org/conference/atc19/presentation/liu-ming}},
}

@inproceedings{sriramanAccelerometerUnderstanding2020,
  title      = {Accelerometer: {{Understanding Acceleration Opportunities}} for {{Data Center Overheads}} at {{Hyperscale}}},
  shorttitle = {Accelerometer},
  booktitle  = {Proceedings of the {{Twenty-Fifth International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author     = {Sriraman, Akshitha and Dhanotia, Abhishek},
  year       = {2020},
  month      = mar,
  series     = {{{ASPLOS}} '20},
  pages      = {733--750},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  doi        = {10.1145/3373376.3378450},
  urldate    = {2025-06-13},
  abstract   = {At global user population scale, important microservices in warehouse-scale data centers can grow to account for an enormous installed base of servers. With the end of Dennard scaling, successive server generations running these microservices exhibit diminishing performance returns. Hence, it is imperative to understand how important microservices spend their CPU cycles to determine acceleration opportunities across the global server fleet. To this end, we first undertake a comprehensive characterization of the top seven microservices that run on the compute-optimized data center fleet at Facebook.Our characterization reveals that microservices spend as few as 18\% of CPU cycles executing core application logic (e.g., performing a key-value store); the remaining cycles are spent in common operations that are not core to the application logic (e.g., I/O processing, logging, and compression). Accelerating such common building blocks can greatly improve data center performance. Whereas developing specialized hardware acceleration for each building block might be beneficial, it becomes risky at scale if these accelerators do not yield expected gains due to performance bounds precipitated by offload-induced overheads. To identify such performance bounds early in the hardware design phase, we develop an analytical model, Accelerometer, for hardware acceleration that projects realistic speedup in microservices. We validate Accelerometer's utility in production using three retrospective case studies and demonstrate how it estimates the real speedup with {$\leq$} 3.7\% error. We then use Accelerometer to project gains from accelerating important common building blocks identified by our characterization.},
  isbn       = {978-1-4503-7102-5},
}
